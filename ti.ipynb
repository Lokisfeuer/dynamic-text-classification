{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Here the binary code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "!pip install openai\n",
    "\n",
    "# importing\n",
    "import sys\n",
    "\n",
    "import numpy as np  # to handle data\n",
    "import pandas as pd  # to handle and save data\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime as d  # to generate timestamps to save models\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer  # for word embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch  # for AI\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import torch.nn.functional as F\n",
    "from torchmetrics import R2Score\n",
    "\n",
    "import matplotlib.pyplot as plt  # to plot training\n",
    "import openai  # to generate training data\n",
    "\n",
    "import seaborn as sns  # to analyse data\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')  # uncomment this line to use the NLTK Downloader\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "class CustomTopicDataset(Dataset):  # the dataset class\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.x = sentences\n",
    "        self.y = labels\n",
    "        if isinstance(self.x, list):\n",
    "            self.length = len(self.x)\n",
    "            self.shape = len(self.x[0])\n",
    "        else:\n",
    "            self.length = self.x.shape[0]\n",
    "            self.shape = self.x[0].shape[0]\n",
    "        self.feature_names = ['sentences', 'labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):  # the NN with linear relu layers and one sigmoid in the end\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack_with_sigmoid = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack_with_sigmoid(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class History:  # The history object to keep track of metrics during training and plot graphs to it.\n",
    "    def __init__(self, val_set, train_set, model, **kwargs):  # kwargs are the metrics to keep track of.\n",
    "        self.val_set = val_set\n",
    "        self.train_set = train_set\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        self.history = {'steps': []}\n",
    "        for i in kwargs.keys():\n",
    "            self.history.update({'val_' + i: []})\n",
    "            self.history.update({'tra_' + i: []})\n",
    "        self.valloader = None\n",
    "        self.trainloader = None\n",
    "\n",
    "    def save(self, step):  # this function is called in the training loop to save the current state of the model.\n",
    "        short_history = {}\n",
    "        for i in self.kwargs.keys():\n",
    "            short_history.update({'val_' + i: []})\n",
    "            short_history.update({'tra_' + i: []})\n",
    "        # generate two dataloader with each k entries from either the training or the validation data.\n",
    "        k = 500\n",
    "        short_train_set, waste = torch.utils.data.random_split(self.train_set, [k, len(self.train_set) - k])\n",
    "        short_val_set, waste = torch.utils.data.random_split(self.val_set, [k, len(self.val_set) - k])\n",
    "        self.valloader = DataLoader(dataset=short_val_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        self.trainloader = DataLoader(dataset=short_train_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        # iterate over both dataloaders simultaneously\n",
    "        for i, ((val_in, val_label), (tra_in, tra_label)) in enumerate(zip(self.valloader, self.trainloader)):\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                # predict outcomes for training and validation.\n",
    "                val_pred = self.model(val_in)\n",
    "                tra_pred = self.model(tra_in)\n",
    "                for j in self.kwargs.keys():  # iterate over the metrics\n",
    "                    # calculate metric and save to short history\n",
    "                    if len(val_pred) > 1:\n",
    "                        val_l = self.kwargs[j](val_pred, val_label).item()\n",
    "                        tra_l = self.kwargs[j](tra_pred, tra_label).item()\n",
    "                        short_history['val_' + j].append(val_l)\n",
    "                        short_history['tra_' + j].append(tra_l)\n",
    "                self.model.train()\n",
    "        # iterate over metrics and save the average of the short history to the history.\n",
    "        for i in self.kwargs.keys():\n",
    "            self.history['val_' + i].append(sum(short_history['val_' + i]) / len(short_history['val_' + i]))\n",
    "            self.history['tra_' + i].append(sum(short_history['tra_' + i]) / len(short_history['tra_' + i]))\n",
    "        self.history['steps'].append(step)  # save steps for the x-axis\n",
    "\n",
    "    # this function is called after training to generate graphs.\n",
    "    # When path is given, the graphs are saved and plt.show() is not called.\n",
    "    def plot(self, path=None):\n",
    "        figures = []\n",
    "        for i in self.kwargs.keys():  # iterate over the metrics and generate graphs for each.\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(self.history['steps'], self.history['val_' + i], 'b')\n",
    "            ax.plot(self.history['steps'], self.history['tra_' + i], 'r')\n",
    "            ax.set_title(i.upper())\n",
    "            ax.set_ylabel(i)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            figures.append(fig)\n",
    "            if path is None:\n",
    "                plt.show()  # depending on the setup the graphs might still be shown even without this function called.\n",
    "            else:\n",
    "                plt.savefig(f\"{path}/{i}\")\n",
    "            plt.clf()\n",
    "        return figures\n",
    "\n",
    "\n",
    "# this function is copied from https://huggingface.co/sentence-transformers/all-roberta-large-v1\n",
    "# it returns embedded versions of the sentences its passed.\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    # test if this works with truncation=False\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "class DYNAMIC_BINARY_AI:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.running_loss = None\n",
    "        self.optimizer = None\n",
    "        self.dataloader = None\n",
    "        self.model = None\n",
    "        self.loss = None\n",
    "        self.dataframe = None\n",
    "        self.val_set = None\n",
    "        self.train_set = None\n",
    "        self.labels = None\n",
    "        self.sentences = None\n",
    "        self.embedded_data = None\n",
    "        self.raw_data = None\n",
    "        self.dataset = None\n",
    "\n",
    "    # generates raw training data; prompt_nr*answer_nr samples are created\n",
    "    def generate_training_data(self, true_prompt, false_prompt, prompt_nr=100, answer_nr=100, load=False):\n",
    "        def ask_ai(prompt, nr):  # get nr of answers from a prompt. Prompt should end with '\\n\\n1.'.\n",
    "            response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=1,\n",
    "                                                max_tokens=10 * nr)\n",
    "            response = '1.' + response['choices'][0]['text'] + '\\n'\n",
    "            li = []\n",
    "            for i in range(nr):\n",
    "                pos = response.find(str(i + 1))\n",
    "                beg = pos + len(str(i + 1)) + 2\n",
    "                end = response[beg:].find('\\n')\n",
    "                li.append(response[beg:beg + end])\n",
    "            return li\n",
    "\n",
    "        def gen_sentences(master_prompt):  # generates nr keywords to the prompt and 50*factor sentences to each\n",
    "            prompt_variations = ask_ai(master_prompt, prompt_nr)\n",
    "            sentences = []\n",
    "            for i in prompt_variations:\n",
    "                sentences.extend(ask_ai(prompt=f'Give me {answer_nr} possible responses to this prompt: \"{i}\"\\n\\n1.', nr=answer_nr))\n",
    "                #with open('sentences_quicksave.json', 'w') as f:\n",
    "                #    f.write(json.dumps({'nr': nr,'all_sentences': all_sentences, 'sentences': sentences}))\n",
    "            return sentences\n",
    "\n",
    "        '''\n",
    "        nr = 0\n",
    "        all_sentences = []\n",
    "        if load and os.path.exists(\"sentences_quicksave.json\"):  # Or folder, will return true or false\n",
    "            with open('sentences_quicksave.json', 'r') as f:\n",
    "                a = json.loads(f.read())\n",
    "            all_sentences = a['all_sentences']\n",
    "            sentences = a['sentences']\n",
    "            nr = a['nr']\n",
    "            if nr == 0:\n",
    "        else:\n",
    "        '''\n",
    "        true_master_prompt = f'Give me {prompt_nr} variations of this prompt: \"{true_prompt}\".\\n\\n1.'\n",
    "        all_sentences = gen_sentences(master_prompt=true_master_prompt)\n",
    "        false_master_prompt = f'Give me {prompt_nr} variations of this prompt: \"{false_prompt}\".\\n\\n1.'\n",
    "        all_sentences.extend(gen_sentences(master_prompt=false_master_prompt))\n",
    "        labels = []\n",
    "        for i in range(len(all_sentences)):\n",
    "            if i < len(all_sentences) / 2:\n",
    "                labels.append(True)\n",
    "            else:\n",
    "                labels.append(False)\n",
    "        data = [all_sentences, labels]\n",
    "        data = np.array(data).transpose()\n",
    "        mapping = []\n",
    "        uni = np.unique(data)\n",
    "        for i in uni:\n",
    "            mapping.append(np.where(data == i)[0][0])\n",
    "        data = data[mapping[1:]]\n",
    "        pd.DataFrame(data).to_csv(f\"{self.name.replace(' ', '_')}_generated_data.csv\", index=False,\n",
    "                                  header=['sentences', 'labels'])\n",
    "        self.raw_data = pd.read_csv(f\"{self.name.replace(' ', '_')}_generated_data.csv\")\n",
    "\n",
    "    # embeds the raw_data\n",
    "    def embed_data(self):\n",
    "        def transpose(lst):\n",
    "            return list(map(list, zip(*lst)))\n",
    "\n",
    "        self.embedded_data = []\n",
    "        k = 100\n",
    "        for i in range(math.ceil(len(self.raw_data['sentences']) / k)):\n",
    "            sentences = long_roberta(list(self.raw_data['sentences'][k * i:k * i + k]))\n",
    "            labels = list(self.raw_data['labels'][k * i:k * i + k])\n",
    "            self.embedded_data.extend(transpose([sentences, labels]))\n",
    "            torch.save(self.embedded_data, f'embedded_data_{self.name}.pt')\n",
    "            print(f'saved {i + 1} / {len(self.raw_data[\"sentences\"]) / k}')\n",
    "        self.to_dataset()\n",
    "\n",
    "    # convert embedded data to a proper dataset\n",
    "    def to_dataset(self):\n",
    "        def transpose(lst):\n",
    "            return list(map(list, zip(*lst)))\n",
    "\n",
    "        self.embedded_data = transpose(self.embedded_data)\n",
    "        self.labels = self.embedded_data[1]\n",
    "        self.sentences = self.embedded_data[0]\n",
    "        self.labels = [torch.tensor([1.]) if i else torch.tensor([0.]) for i in self.labels]\n",
    "        self.dataset = CustomTopicDataset(self.sentences, self.labels)\n",
    "\n",
    "    # analyse the data including: balance, common words (wordcloud), sample lengths, duplicates and null values\n",
    "    def analyse_training_data(self):\n",
    "        print('Analysing training data...')\n",
    "        print('General information')\n",
    "        self.raw_data.info()\n",
    "        self.raw_data.groupby(['labels']).describe()\n",
    "        print(f'Number of unique sentences: {self.raw_data[\"sentences\"].nunique()}')\n",
    "        duplicates = self.raw_data[self.raw_data.duplicated()]\n",
    "        print(f'Number of duplicate rows:\\n{len(duplicates)}')\n",
    "        print(f'Check for null values:\\n{self.raw_data.isnull().sum()}')\n",
    "        sns.countplot(x=self.raw_data['labels'])  # plotting distribution for easier understanding\n",
    "        # TODO: check with colabs position of plot\n",
    "        print('The start of the dataset:')\n",
    "        print(self.raw_data.head(3))\n",
    "\n",
    "        print('A few random examples from the dataset:')\n",
    "        # let's see how data is looklike\n",
    "        random_index = random.randint(0, self.raw_data.shape[0] - 3)\n",
    "        for row in self.raw_data[['sentences', 'labels']][random_index:random_index + 3].itertuples():\n",
    "            _, text, label = row\n",
    "            print(f'TEXT: {text}')\n",
    "            print(f'LABEL: {label}')\n",
    "\n",
    "        true_data = self.raw_data[self.raw_data['labels'] == 1]\n",
    "        true_data = true_data['sentences']\n",
    "        false_data = self.raw_data[self.raw_data['labels'] == 0]\n",
    "        false_data = false_data['sentences']\n",
    "\n",
    "        def wordcloud_draw(data, color, s):\n",
    "            words = ' '.join(data)\n",
    "            cleaned_word = \" \".join([word for word in words.split() if (word != 'movie' and word != 'film')])\n",
    "            wordcloud = WordCloud(stopwords=stopwords.words('english'), background_color=color, width=2500,\n",
    "                                  height=2000).generate(cleaned_word)\n",
    "            plt.imshow(wordcloud)\n",
    "            plt.title(s)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.figure(figsize=[20, 10])  # first value is to the side, second is height.\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        wordcloud_draw(true_data, 'white', 'Most-common words about the topic')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        wordcloud_draw(false_data, 'white', 'Most-common words not about the topic')\n",
    "        plt.show()  # end wordcloud\n",
    "\n",
    "        self.raw_data['text_word_count'] = self.raw_data['sentences'].apply(lambda x: len(x.split()))\n",
    "\n",
    "        plt.figure(figsize=(15, 10))  # plt.figure(figsize=(20, 3))\n",
    "        plt.subplot(1, 1, 1)  # plt.subplot(1, 3, i + 1)\n",
    "        sns.histplot(data=self.raw_data, x='text_word_count', hue='labels', bins=50)\n",
    "        plt.title(f\"Distirbution of Various word counts with respect to target\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # this function trains a model and returns it as well as the history object of its training process.\n",
    "    def train(self, epochs=10, lr=0.001, val_frac=0.1, batch_size=25, loss=nn.BCELoss()):\n",
    "        # get_acc measures the accuracy and is passed as a metric to the history object.\n",
    "        def get_acc(pred, target):\n",
    "            pred_tag = torch.round(pred)\n",
    "\n",
    "            correct_results_sum = (pred_tag == target).sum().float()\n",
    "            acc = correct_results_sum / target.shape[0]\n",
    "            acc = torch.round(acc * 100)\n",
    "\n",
    "            return acc\n",
    "\n",
    "        # generate validation dataset with the fraction of entries of the full set passed as val_frac\n",
    "        val_len = int(round(len(self.dataset) * val_frac))\n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(self.dataset,\n",
    "                                                                     [len(self.dataset) - val_len, val_len])\n",
    "        self.dataloader = DataLoader(dataset=self.train_set, batch_size=batch_size, shuffle=True)\n",
    "        self.model = NeuralNetwork(self.dataset.shape)\n",
    "\n",
    "        self.loss = loss  # the loss passed to this train function\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        # define metrics to be monitored by the history object during training.\n",
    "        r2loss = R2Score()\n",
    "        mseloss = nn.MSELoss()\n",
    "        bceloss = nn.BCELoss()\n",
    "        accuracy = get_acc\n",
    "\n",
    "        history = History(self.val_set, self.train_set, self.model, r2loss=r2loss, mseloss=mseloss, accuracy=accuracy,\n",
    "                          bceloss=bceloss)  # define history object\n",
    "\n",
    "        # main training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.running_loss = []\n",
    "            print(f'Starting new epoch {epoch + 1}/{epochs}')\n",
    "            for step, (inputs, labels) in enumerate(self.dataloader):\n",
    "                y_pred = self.model(inputs)\n",
    "                lo = self.loss(y_pred, labels)\n",
    "                lo.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.running_loss.append(lo.item())\n",
    "                if (step + 1) % math.floor(len(self.dataloader) / 5 + 2) == 0:  # if (step+1) % 100 == 0:\n",
    "                    print(f'current loss:\\t\\t{sum(self.running_loss) / len(self.running_loss)}')\n",
    "                    self.running_loss = []\n",
    "                    history.save(epoch + step / len(self.dataloader))\n",
    "                    # save current state of the model to history\n",
    "        # generate folder with timestamp and save the model there.\n",
    "        now = str(d.now().isoformat()).replace(':', 'I').replace('.', 'i').replace('-', '_')\n",
    "        os.mkdir(f\"model_{now}\")\n",
    "        torch.save(self.model, f\"model_{now}/model.pt\")\n",
    "        print(f'Model saved to \"model_{now}/model.pt\"')\n",
    "        history.plot(f\"model_{now}\")  # save graphs to the folder\n",
    "        return history, self.model  # return history and model\n",
    "\n",
    "\n",
    "# with this function you can pass custom sentences to the model\n",
    "def try_model(model):\n",
    "    a = input('Please enter your input sentence: ')\n",
    "    a = long_roberta(a)\n",
    "    pred = model(a)\n",
    "    print(pred.item())\n",
    "    print('Where 1 is the first, true prompt: \"\"\\nand 0 is the second, false prompt: \"\".\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ti = DYNAMIC_BINARY_AI('topic_identifier')\n",
    "    true_prompt = 'Write a short question about biology.'\n",
    "    false_prompt = 'Write a short factual statement about shakespeare.'\n",
    "    ti.generate_training_data(true_prompt, false_prompt, prompt_nr=10, answer_nr=100)\n",
    "    #ti.raw_data = pd.read_csv(f\"pre_prepared_data/topic_identifier_generated_data.csv\")\n",
    "    print('ANALYSE DATA BEGINN')\n",
    "    ti.analyse_training_data()\n",
    "    print('ANALYSE DATA END')\n",
    "    ti.embed_data()\n",
    "    history, model = ti.train(epochs=10, lr=0.0001, val_frac=0.1, batch_size=10, loss=nn.BCELoss())\n",
    "    history.plot()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here the multi choice code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentence_transformers\n",
    "!pip install torchmetrics\n",
    "!pip install torcheval\n",
    "!pip install openai\n",
    "\n",
    "# importing\n",
    "import sys\n",
    "\n",
    "import numpy as np  # to handle data\n",
    "import pandas as pd  # to handle and save data\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime as d  # to generate timestamps to save models\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer  # for word embedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch  # for AI\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import R2Score\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import matplotlib.pyplot as plt  # to plot training\n",
    "import openai  # to generate training data\n",
    "\n",
    "import seaborn as sns  # to analyse data\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')  # uncomment this line to use the NLTK Downloader\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "class CustomTopicDataset(Dataset):  # the dataset class\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.x = sentences\n",
    "        self.y = labels\n",
    "        if isinstance(self.x, list):\n",
    "            self.length = len(self.x)\n",
    "            self.shape = len(self.x[0])\n",
    "        else:\n",
    "            self.length = self.x.shape[0]\n",
    "            self.shape = self.x[0].shape[0]\n",
    "        self.feature_names = ['sentences', 'labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):  # the NN with linear relu layers and one sigmoid in the end\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return F.softmax(logits)\n",
    "\n",
    "\n",
    "class History:  # The history object to keep track of metrics during training and plot graphs to it.\n",
    "    def __init__(self, val_set, train_set, model, **kwargs):  # kwargs are the metrics to keep track of.\n",
    "        self.val_set = val_set\n",
    "        self.train_set = train_set\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        self.history = {'steps': []}\n",
    "        for i in kwargs.keys():\n",
    "            self.history.update({'val_' + i: []})\n",
    "            self.history.update({'tra_' + i: []})\n",
    "        self.valloader = None\n",
    "        self.trainloader = None\n",
    "\n",
    "    def save(self, step):  # this function is called in the training loop to save the current state of the model.\n",
    "        short_history = {}\n",
    "        for i in self.kwargs.keys():\n",
    "            short_history.update({'val_' + i: []})\n",
    "            short_history.update({'tra_' + i: []})\n",
    "        # generate two dataloader with each k entries from either the training or the validation data.\n",
    "        k = 500\n",
    "        short_train_set, waste = torch.utils.data.random_split(self.train_set, [k, len(self.train_set) - k])\n",
    "        short_val_set, waste = torch.utils.data.random_split(self.val_set, [k, len(self.val_set) - k])\n",
    "        self.valloader = DataLoader(dataset=short_val_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        self.trainloader = DataLoader(dataset=short_train_set, batch_size=5, shuffle=True, num_workers=2)\n",
    "        # iterate over both dataloaders simultaneously\n",
    "        for i, ((val_in, val_label), (tra_in, tra_label)) in enumerate(zip(self.valloader, self.trainloader)):\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                # predict outcomes for training and validation.\n",
    "                val_pred = self.model(val_in)\n",
    "                tra_pred = self.model(tra_in)\n",
    "                for j in self.kwargs.keys():  # iterate over the metrics\n",
    "                    # calculate metric and save to short history\n",
    "                    if len(val_pred) > 1:\n",
    "                        val_l = self.kwargs[j](val_pred, val_label).item()\n",
    "                        tra_l = self.kwargs[j](tra_pred, tra_label).item()\n",
    "                        short_history['val_' + j].append(val_l)\n",
    "                        short_history['tra_' + j].append(tra_l)\n",
    "                self.model.train()\n",
    "        # iterate over metrics and save the average of the short history to the history.\n",
    "        for i in self.kwargs.keys():\n",
    "            self.history['val_' + i].append(sum(short_history['val_' + i]) / len(short_history['val_' + i]))\n",
    "            self.history['tra_' + i].append(sum(short_history['tra_' + i]) / len(short_history['tra_' + i]))\n",
    "        self.history['steps'].append(step)  # save steps for the x-axis\n",
    "\n",
    "    # this function is called after training to generate graphs.\n",
    "    # When path is given, the graphs are saved and plt.show() is not called.\n",
    "    def plot(self, path=None):\n",
    "        figures = []\n",
    "        for i in self.kwargs.keys():  # iterate over the metrics and generate graphs for each.\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(self.history['steps'], self.history['val_' + i], 'b')\n",
    "            ax.plot(self.history['steps'], self.history['tra_' + i], 'r')\n",
    "            ax.set_title(i.upper())\n",
    "            ax.set_ylabel(i)\n",
    "            ax.set_xlabel('Epochs')\n",
    "            figures.append(fig)\n",
    "            if path is None:\n",
    "                plt.show()  # depending on the setup the graphs might still be shown even without this function called.\n",
    "            else:\n",
    "                plt.savefig(f\"{path}/{i}\")\n",
    "            plt.clf()\n",
    "        return figures\n",
    "\n",
    "\n",
    "# this function is copied from https://huggingface.co/sentence-transformers/all-roberta-large-v1\n",
    "# it returns embedded versions of the sentences its passed.\n",
    "def long_roberta(sentences):\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Sentences we want sentence embeddings for\n",
    "    # sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    # test if this works with truncation=False\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "class DYNAMIC_MULTI_CLASS_AI:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.running_loss = None\n",
    "        self.optimizer = None\n",
    "        self.dataloader = None\n",
    "        self.model = None\n",
    "        self.loss = None\n",
    "        self.dataframe = None\n",
    "        self.val_set = None\n",
    "        self.train_set = None\n",
    "        self.labels = None\n",
    "        self.sentences = None\n",
    "        self.embedded_data = None\n",
    "        self.raw_data = None\n",
    "        self.dataset = None\n",
    "\n",
    "    # generates raw training data; prompt_nr*answer_nr samples are created\n",
    "    def generate_training_data(self, prompt_nr=100, answer_nr=100, load=False, **prompts):\n",
    "        def ask_ai(prompt):  # get nr of answers from a prompt. Prompt should end with '\\n\\n1.'.\n",
    "            response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=1,\n",
    "                                                max_tokens=10 * answer_nr)\n",
    "            response = '1.' + response['choices'][0]['text'] + '\\n'\n",
    "            li = []\n",
    "            for i in range(answer_nr):\n",
    "                pos = response.find(str(i + 1))\n",
    "                beg = pos + len(str(i + 1)) + 2\n",
    "                end = response[beg:].find('\\n')\n",
    "                li.append(response[beg:beg + end])\n",
    "            return li\n",
    "\n",
    "        def gen_sentences():  # generates nr keywords to the prompt and 50*factor sentences to each\n",
    "            prompt_variations = ask_ai(master_prompt)\n",
    "            sentences = []\n",
    "            for i in prompt_variations:\n",
    "                sentences.extend(ask_ai(prompt=f'Give me {answer_nr} possible responses to this prompt: \"{i}\"\\n\\n1.'))\n",
    "            return sentences\n",
    "\n",
    "        if load and os.path.exists(\"sentences_quicksave.json\"):\n",
    "            with open('generation_quicksave.json', 'r') as f:\n",
    "                a = json.loads(f.read())\n",
    "            labels = a['labels']\n",
    "            all_sentences = a['all_sentences']\n",
    "            x = a['x']\n",
    "        else:\n",
    "            labels = []\n",
    "            all_sentences = []\n",
    "            x = 0\n",
    "        for i in list(prompts.keys())[x:]:\n",
    "            master_prompt = f'Give me {prompt_nr} variations of this prompt: \"{prompts[i]}\".\\n\\n1.'\n",
    "            new_sentences = gen_sentences()\n",
    "            all_sentences.extend(new_sentences)\n",
    "            labels.extend([x] * len(new_sentences))\n",
    "            x += 1\n",
    "            with open('generation_quicksave.json', 'w') as f:\n",
    "                f.write(json.dumps({'labels': labels, 'all_sentences': all_sentences, 'x': x}))\n",
    "\n",
    "        data = [all_sentences, labels]\n",
    "        data = np.array(data).transpose()\n",
    "        mapping = []\n",
    "        uni = np.unique(data)\n",
    "        for i in uni:\n",
    "            mapping.append(np.where(data == i)[0][0])\n",
    "        data = data[mapping[1:]]\n",
    "        pd.DataFrame(data).to_csv(f\"{self.name.replace(' ', '_')}_generated_data.csv\", index=False,\n",
    "                                  header=['sentences', 'labels'])\n",
    "        self.raw_data = pd.read_csv(f\"{self.name.replace(' ', '_')}_generated_data.csv\")\n",
    "\n",
    "    # embeds the raw_data\n",
    "    def embed_data(self):\n",
    "        def transpose(lst):\n",
    "            return list(map(list, zip(*lst)))\n",
    "\n",
    "        self.embedded_data = []\n",
    "        k = 100\n",
    "        for i in range(math.ceil(len(self.raw_data['sentences']) / k)):\n",
    "            sentences = long_roberta(list(self.raw_data['sentences'][k * i:k * i + k]))\n",
    "            labels = list(self.raw_data['labels'][k * i:k * i + k])\n",
    "            self.embedded_data.extend(transpose([sentences, labels]))\n",
    "            torch.save(self.embedded_data, f'embedded_data_{self.name}.pt')\n",
    "            print(f'saved {i + 1} / {len(self.raw_data[\"sentences\"]) / k}')\n",
    "        self.to_dataset()\n",
    "\n",
    "    # convert embedded data to a proper dataset\n",
    "    def to_dataset(self):\n",
    "        def transpose(lst):\n",
    "            return list(map(list, zip(*lst)))\n",
    "\n",
    "        def onehot(idx):\n",
    "            one_hot = np.zeros(self.raw_data[\"labels\"].nunique())\n",
    "            one_hot[idx] = 1\n",
    "            return torch.from_numpy(one_hot)\n",
    "\n",
    "        self.embedded_data = transpose(self.embedded_data)\n",
    "        self.labels = self.embedded_data[1]\n",
    "        self.sentences = self.embedded_data[0]\n",
    "        # self.labels = [torch.tensor([i]) for i in self.labels]\n",
    "        # self.labels = [onehot(i) for i in self.labels]\n",
    "        self.dataset = CustomTopicDataset(self.sentences, self.labels)\n",
    "\n",
    "    # analyse the data including: balance, common words (wordcloud), sample lengths, duplicates and null values\n",
    "    def analyse_training_data(self):\n",
    "        print('Analysing training data...')\n",
    "        print('General information')\n",
    "        self.raw_data.info()\n",
    "        self.raw_data.groupby(['labels']).describe()\n",
    "        print(f'Number of unique sentences: {self.raw_data[\"sentences\"].nunique()}')\n",
    "        duplicates = self.raw_data[self.raw_data.duplicated()]\n",
    "        print(f'Number of duplicate rows:\\n{len(duplicates)}')\n",
    "        print(f'Check for null values:\\n{self.raw_data.isnull().sum()}')\n",
    "        sns.countplot(x=self.raw_data['labels'])  # plotting distribution for easier understanding\n",
    "        # TODO: check with colabs position of plot\n",
    "        print('The start of the dataset:')\n",
    "        print(self.raw_data.head(3))\n",
    "\n",
    "        print('A few random examples from the dataset:')\n",
    "        # let's see how data is looklike\n",
    "        random_index = random.randint(0, self.raw_data.shape[0] - 3)\n",
    "        for row in self.raw_data[['sentences', 'labels']][random_index:random_index + 3].itertuples():\n",
    "            _, text, label = row\n",
    "            print(f'TEXT: {text}')\n",
    "            print(f'LABEL: {label}')\n",
    "\n",
    "        def wordcloud_draw(data, color, s):\n",
    "            words = ' '.join(data)\n",
    "            cleaned_word = \" \".join([word for word in words.split() if (word != 'movie' and word != 'film')])\n",
    "            wordcloud = WordCloud(stopwords=stopwords.words('english'), background_color=color, width=2500,\n",
    "                                  height=2000).generate(cleaned_word)\n",
    "            plt.imshow(wordcloud)\n",
    "            plt.title(s)\n",
    "            plt.axis('off')\n",
    "\n",
    "        x = self.raw_data[\"labels\"].nunique()\n",
    "        plt.figure(figsize=[10*x, 10])\n",
    "        for i in range(x):\n",
    "            part_data = self.raw_data[self.raw_data[\"labels\"] == i]\n",
    "            part_data = part_data['sentences']\n",
    "            plt.subplot(1, x, 1 + i)\n",
    "            wordcloud_draw(part_data, 'white', 'Most-common words about the topic')\n",
    "        plt.show()\n",
    "\n",
    "        self.raw_data['text_word_count'] = self.raw_data['sentences'].apply(lambda x: len(x.split()))\n",
    "\n",
    "        plt.figure(figsize=(15, 10))  # plt.figure(figsize=(20, 3))\n",
    "        plt.subplot(1, 1, 1)  # plt.subplot(1, 3, i + 1)\n",
    "        sns.histplot(data=self.raw_data, x='text_word_count', hue='labels', bins=50)\n",
    "        plt.title(f\"Distirbution of Various word counts with respect to target\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # this function trains a model and returns it as well as the history object of its training process.\n",
    "    def train(self, epochs=10, lr=0.001, val_frac=0.1, batch_size=25, loss=nn.CrossEntropyLoss(), **metrics):\n",
    "        # get_acc measures the accuracy and is passed as a metric to the history object.\n",
    "        def get_acc(pred, target):\n",
    "            pred_tag = torch.argmax(pred, dim=1)\n",
    "\n",
    "            correct_results_sum = torch.sum(torch.eq(pred_tag, target))\n",
    "            acc = correct_results_sum / target.shape[0]\n",
    "            acc = acc * 100\n",
    "\n",
    "            return acc\n",
    "\n",
    "        # generate validation dataset with the fraction of entries of the full set passed as val_frac\n",
    "        val_len = int(round(len(self.dataset) * val_frac))\n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(self.dataset,\n",
    "                                                                     [len(self.dataset) - val_len, val_len])\n",
    "        self.dataloader = DataLoader(dataset=self.train_set, batch_size=batch_size, shuffle=True)\n",
    "        self.model = NeuralNetwork(self.dataset.shape, self.raw_data[\"labels\"].nunique())\n",
    "\n",
    "        self.loss = loss  # the loss passed to this train function\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "\n",
    "        # define metrics to be monitored by the history object during training.\n",
    "        # r2loss = R2Score()\n",
    "        mseloss = nn.MSELoss()\n",
    "        # bceloss = nn.BCELoss()\n",
    "        accuracy = get_acc\n",
    "        cel = nn.CrossEntropyLoss()\n",
    "\n",
    "        # define history object\n",
    "        history = History(self.val_set, self.train_set, self.model, accuracy=accuracy, cross_entropy_loss=cel, f1_score=multiclass_f1_score, **metrics)\n",
    "\n",
    "        # main training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.running_loss = []\n",
    "            print(f'Starting new epoch {epoch + 1}/{epochs}')\n",
    "            for step, (inputs, labels) in enumerate(self.dataloader):\n",
    "                y_pred = self.model(inputs)\n",
    "                lo = self.loss(y_pred, labels)\n",
    "                lo.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.running_loss.append(lo.item())\n",
    "                if (step + 1) % math.floor(len(self.dataloader) / 5 + 2) == 0:  # if (step+1) % 100 == 0:\n",
    "                    print(f'current loss:\\t\\t{sum(self.running_loss) / len(self.running_loss)}')\n",
    "                    self.running_loss = []\n",
    "                    history.save(epoch + step / len(self.dataloader))\n",
    "                    # save current state of the model to history\n",
    "        # generate folder with timestamp and save the model there.\n",
    "        now = str(d.now().isoformat()).replace(':', 'I').replace('.', 'i').replace('-', '_')\n",
    "        os.mkdir(f\"model_{now}\")\n",
    "        torch.save(self.model, f\"model_{now}/model.pt\")\n",
    "        print(f'Model saved to \"model_{now}/model.pt\"')\n",
    "        history.plot(f\"model_{now}\")  # save graphs to the folder\n",
    "        return history, self.model  # return history and model\n",
    "\n",
    "\n",
    "# with this function you can pass custom sentences to the model\n",
    "def try_model(model):\n",
    "    a = input('Please enter your input sentence: ')\n",
    "    a = long_roberta(a)\n",
    "    pred = model(a)\n",
    "    print(pred)\n",
    "    print(f'Final ')\n",
    "    print('The probabilities for each prompt in order.')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "    model = torch.load('model_2023_07_22T10I44I05i062759/model.pt')\n",
    "    while True:\n",
    "        try_model(model)\n",
    "    '''\n",
    "    gbc = DYNAMIC_MULTI_CLASS_AI('geo_bio_cook')\n",
    "    true_prompt = 'Write a short factual statement about geology.'\n",
    "    false_prompt = 'Write a short factual statement about biology.'\n",
    "    cook_prompt = 'Write a short factual statement about cooking.'\n",
    "    gbc.generate_training_data(prompt_nr=10, answer_nr=50, load=True, geo_prompt=true_prompt, shakespeare_prompt=false_prompt, cook_prompt=cook_prompt)\n",
    "    #gbc.raw_data = pd.read_csv(f\"pre_prepared_data/geo_bio_cook_generated_data.csv\")\n",
    "    print('ANALYSE DATA BEGINN')\n",
    "    gbc.analyse_training_data()\n",
    "    print('ANALYSE DATA END')\n",
    "    gbc.embed_data()\n",
    "    #gbc.embedded_data = torch.load(f'pre_prepared_data/embedded_data_geo_bio_cook.pt')\n",
    "    #gbc.to_dataset()\n",
    "    history, model = gbc.train(epochs=25, lr=0.07, val_frac=0.1, batch_size=25, loss=nn.CrossEntropyLoss())\n",
    "    history.plot()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
